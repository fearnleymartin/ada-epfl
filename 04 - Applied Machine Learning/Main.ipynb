{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and simple cleaning of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, some simple cleaning. We remove features that have nothing to do with the skin color. We remove playerShort (we will use 'player' later for aggregating), birthday, Alpha_3 (since it is the same as refCountry) and photoID.\n",
    "\n",
    "There are missing values for height, weight and position. And also 163 dyads miss the information for the implicit association test and the explicit bias scores.\n",
    "\n",
    "First we will remove the refrees that got in the data by mistake (as mentioned in the preprocessing article) and see if the problem is solved then. After removing these refrees, there were still 110 samples with missing data which we just removed.\n",
    "\n",
    "The missing values of height and weight are replaced by the respective means and the missing values in position are replaced by the most frequently occuring position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv ('CrowdstormingDataJuly1st.csv')\n",
    "df.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove values without skin color rating (we know all samples have either two raters or none)\n",
    "df_train_raw = df [pd.notnull (df ['rater1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop unimportant features\n",
    "df_train_raw = df_train_raw.drop(['birthday', 'player', 'Alpha_3', 'photoID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove referees that are not supposed to be in here\n",
    "refrees_to_remove = df_train_raw.groupby('refNum').sum()[df_train_raw.groupby('refNum').sum()['games'] < 22].index.tolist()\n",
    "df_train_raw = df_train_raw[~df_train_raw.refNum.isin(refrees_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove samples that don't have IAT or Exp score information\n",
    "df_train_raw = df_train_raw[df_train_raw.meanIAT.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set missing height, weight and position values to mean\n",
    "mean_height = df_train_raw.height.mean()\n",
    "mean_weight = df_train_raw.weight.mean()\n",
    "most_frequent_position = df_train_raw['position'].value_counts().index[0]\n",
    "\n",
    "df_train_raw.loc[df_train_raw.height.isnull(),'height'] = mean_height\n",
    "df_train_raw.loc[df_train_raw.weight.isnull(),'weight'] = mean_weight\n",
    "df_train_raw['position'] = df_train_raw['position'].fillna(most_frequent_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new feature called \"skinColor\" which is the average of both ratings, then mapped to either \"white\" or \"black\" to simplify our classification.\n",
    "\n",
    "Position, club and leagueCountry are not numerical, we use dummy variables to make them numerical. For now we left out the club feature because it would cause a lot of dummy variables and we assume it is not a significant feature. We will check in the end if we get significantly better result if we add this feature.\n",
    "\n",
    "We drop height and weight, as there are not features that we are looking for to influence the model.\n",
    "\n",
    "**TODO: Aggregrate per player** (Done)\n",
    "\n",
    "**TODO: feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_train_raw\n",
    "\n",
    "# create feature \"skinColor\"\n",
    "def attribute_skin_label(val):\n",
    "    if val > 0.5:\n",
    "        return 1 #  \"black\"\n",
    "    else:\n",
    "        return 0  # \"white\"\n",
    "#Replace rating with either white or black\n",
    "df_train['skinColor'] = df_train[['rater1','rater2']].mean(axis=1).apply(attribute_skin_label)\n",
    "df_train = df_train.drop(['rater1', 'rater2'], 1)\n",
    "\n",
    "# raters = pd.DataFrame (df_train_raw [['rater1', 'rater2']].mean (axis=1))\n",
    "# raters.columns = ['skinColor']\n",
    "# df_train = pd.merge (raters, df_train_raw.drop (['rater1', 'rater2'], 1), right_index = True, left_index = True)\n",
    "\n",
    "# add dummy variables for position, club and leagueCountry\n",
    "n_positions = len(df_train.position.unique())\n",
    "#n_club = len(df_train.club.unique())\n",
    "n_leagueCountry = len(df_train.leagueCountry.unique())\n",
    "\n",
    "d_positions = pd.get_dummies(df_train['position'])\n",
    "#d_club = pd.get_dummies(df_train['club'])\n",
    "d_leagueCountry = pd.get_dummies(df_train['leagueCountry'])\n",
    "\n",
    "df_train = pd.concat([df_train, d_positions, d_leagueCountry], axis=1)\n",
    "df_train = df_train.drop(['position', 'club', 'leagueCountry','height','weight'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking how many \"white\" and \"black\" players we have\n",
    "df_train['skinColor'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player aggregation\n",
    "\n",
    "We aggregate per player because we want to predict the skin color of a given player, not of a given dyad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['IATRedCards'] = df_train['redCards']*df_train['meanIAT']\n",
    "df_train['ExpRedCards'] = df_train['redCards']*df_train['meanExp']\n",
    "df_train['IATYellowRedCards'] = df_train['yellowReds']*df_train['meanIAT']\n",
    "df_train['ExpYellowRedCards'] = df_train['yellowReds']*df_train['meanExp']\n",
    "df_train['IATYellowCards'] = df_train['yellowCards']*df_train['meanIAT']\n",
    "df_train['ExpYellowCards'] = df_train['yellowCards']*df_train['meanExp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_agg = df_train.groupby('playerShort').sum().columns\n",
    "summed = df_train[['playerShort','games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']].groupby('playerShort').sum()\n",
    "meaned = df_train[['playerShort', 'skinColor','Attacking Midfielder', 'Center Back', 'Center Forward',\n",
    "       'Center Midfielder', 'Defensive Midfielder', 'Goalkeeper',\n",
    "       'Left Fullback', 'Left Midfielder', 'Left Winger', 'Right Fullback',\n",
    "       'Right Midfielder', 'Right Winger', 'England', 'France', 'Germany',\n",
    "       'Spain', 'meanIAT', 'meanExp', 'IATRedCards', 'ExpRedCards', 'IATYellowRedCards', 'ExpYellowRedCards', 'IATYellowCards', 'ExpYellowCards' ]].groupby('playerShort').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_agg = pd.concat([summed, meaned], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_agg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the model\n",
    "\n",
    "\n",
    "We decide to keep meanIAT and meanExp. As we have aggregated by player, the represent the average amount of bias the players receive over all the referees they meet. \n",
    "We decide to drop refNum, refCountry, nIAT, seIAT, nExp, seExp for simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "Y = np.asarray(df_agg['skinColor'], dtype='str')\n",
    "X = df_agg.drop(['skinColor','refNum','refCountry','nIAT','seIAT','nExp','seExp'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.columns  # Just checking the columns we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n",
    "We check the accuracy of our model through cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, X, Y, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "We check what the most important features are.\n",
    "TODO: graph of feature importance distribution\n",
    "\n",
    "We see that the \"average bias\" the players encounter from referees is the most important factor in determining skin color.\n",
    "\n",
    "Also the position they play in has very little relevance.\n",
    "\n",
    "The number of cards they receive is somewhat relevant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importance = {}\n",
    "for x,y in zip(X.columns,clf.feature_importances_):\n",
    "    feature_importance[x]=y\n",
    "import operator\n",
    "sorted_feature_importance = sorted(feature_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
    "for i in sorted_feature_importance:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(title, estimator, X, y, cv=20):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "        \n",
    "    estimator: clf\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "title = \"Learning Curves (RandomForestClassifier)\"\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=10)\n",
    "Y = np.asarray(df_agg['skinColor'], dtype='str')\n",
    "X = df_agg.drop(['skinColor','refNum','refCountry','nIAT','seIAT','nExp','seExp'], 1)\n",
    "\n",
    "plot_learning_curve(title, estimator, X, Y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation score is significanty worse than the training score and does not improve when adding more data. This means our model has a large bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Learning Curves (RandomForestClassifier) - combined features\"\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=10)\n",
    "Y = np.asarray(df_agg['skinColor'], dtype='str')\n",
    "X = df_agg[['yellowCards','yellowReds', 'redCards', 'Attacking Midfielder','Center Back', 'Center Forward', 'Center Midfielder','Defensive Midfielder', 'Goalkeeper', 'Left Fullback','Left Midfielder', 'Left Winger', 'Right Fullback', 'Right Midfielder', 'Right Winger', 'England', 'France', 'Germany', 'Spain', 'meanIAT', 'meanExp', 'IATRedCards', 'ExpRedCards', 'IATYellowRedCards', 'ExpYellowRedCards', 'IATYellowCards', 'ExpYellowCards']]\n",
    "\n",
    "plot_learning_curve(title, estimator, X, Y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried to combine the IAT/Exp information with the red cards. Same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Learning Curves (RandomForestClassifier) - Only using position\"\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=10)\n",
    "Y = np.asarray(df_agg['skinColor'], dtype='str')\n",
    "X = df_agg[['Attacking Midfielder','Center Back', 'Center Forward', 'Center Midfielder','Defensive Midfielder', 'Goalkeeper', 'Left Fullback','Left Midfielder', 'Left Winger', 'Right Fullback', 'Right Midfielder', 'Right Winger']]\n",
    "\n",
    "plot_learning_curve(title, estimator, X, Y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I was trying to get a bad score by only using the position dummy variables. I'm not sure what to think of these results. The first two graphs look like high bias but might also be overfitting because the difference between test and train is significant while both scores seperately are alright. This last graph might comfirm this because it has low complexity (so definitaly not overfitting) and here the curves are close together around the 80% (the same as the cross-validation score of the complexer model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
