{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and simple cleaning of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, some simple cleaning. We remove features that have nothing to do with the skin color. We remove player (we will use 'playerShort' later for aggregating), birthday, Alpha_3 (since it is the same as refCountry) and photoID.\n",
    "\n",
    "There are missing values for height, weight and position. And also 163 dyads miss the information for the implicit association test and the explicit bias scores.\n",
    "\n",
    "First we will remove the referees that got in the data by mistake (as mentioned in the preprocessing article) and see if the problem is solved then. After removing these referees, there were still 110 samples with missing data which we just removed.\n",
    "\n",
    "The missing values of height and weight are replaced by the respective means and the missing values in position are replaced by the most frequently occuring position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv ('CrowdstormingDataJuly1st.csv')\n",
    "df.head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove values without skin color rating (we know all samples have either two raters or none)\n",
    "df_train_raw = df [pd.notnull (df ['rater1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop unimportant features\n",
    "df_train_raw = df_train_raw.drop(['birthday', 'player', 'Alpha_3', 'photoID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove referees that are not supposed to be in here\n",
    "referees_to_remove = df_train_raw.groupby('refNum').sum()[df_train_raw.groupby('refNum').sum()['games'] < 22].index.tolist()\n",
    "df_train_raw = df_train_raw[~df_train_raw.refNum.isin(referees_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove samples that don't have IAT or Exp score information\n",
    "df_train_raw = df_train_raw[df_train_raw.meanIAT.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set missing height, weight and position values to mean\n",
    "mean_height = df_train_raw.height.mean()\n",
    "mean_weight = df_train_raw.weight.mean()\n",
    "most_frequent_position = df_train_raw['position'].value_counts().index[0]\n",
    "\n",
    "df_train_raw.loc[df_train_raw.height.isnull(),'height'] = mean_height\n",
    "df_train_raw.loc[df_train_raw.weight.isnull(),'weight'] = mean_weight\n",
    "df_train_raw['position'] = df_train_raw['position'].fillna(most_frequent_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new feature called \"skinColor\" which is the average of both ratings, then mapped to either \"white\" or \"black\" to simplify our classification.\n",
    "\n",
    "Position, club and leagueCountry are not numerical, we use dummy variables to make them numerical. For now we left out the club feature because it would cause a lot of dummy variables and we assume it is not a significant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_train_raw\n",
    "\n",
    "# create feature \"skinColor\"\n",
    "def attribute_skin_label(val):\n",
    "    if val > 0.5:\n",
    "        return 1 #  \"black\"\n",
    "    else:\n",
    "        return 0  # \"white\"\n",
    "    \n",
    "#Replace rating with either white or black\n",
    "df_train['skinColor'] = df_train[['rater1','rater2']].mean(axis=1).apply(attribute_skin_label)\n",
    "df_train = df_train.drop(['rater1', 'rater2'], 1)\n",
    "\n",
    "# add dummy variables for position and leagueCountry\n",
    "n_positions = len(df_train.position.unique())\n",
    "n_leagueCountry = len(df_train.leagueCountry.unique())\n",
    "\n",
    "d_positions = pd.get_dummies(df_train['position'])\n",
    "d_leagueCountry = pd.get_dummies(df_train['leagueCountry'])\n",
    "\n",
    "df_train = pd.concat([df_train, d_positions, d_leagueCountry], axis=1)\n",
    "df_train = df_train.drop(['position', 'club', 'leagueCountry'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking how many \"white\" and \"black\" players we have\n",
    "df_train['skinColor'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player aggregation and feature engineering\n",
    "We added 6 new features: the meanIAT and the meanExp per dyad are multiplied with each of the values 'redCards', 'YellowCards', and 'YellowRedCards'. We do this because we assume that in a specific encounter between a referee and a player, the combination of a given card and the bias score of the referee's country, is a large indicator for darker skin color.\n",
    "\n",
    "We aggregate per player because we want to predict the skin color of a given player, not of a given dyad. To obtain an intuitive result, we choose to sum over values such as games, games outcomes and cards and mean over the dummy variables and bias scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['IATRedCards'] = df_train['redCards']*df_train['meanIAT']\n",
    "df_train['ExpRedCards'] = df_train['redCards']*df_train['meanExp']\n",
    "df_train['IATYellowRedCards'] = df_train['yellowReds']*df_train['meanIAT']\n",
    "df_train['ExpYellowRedCards'] = df_train['yellowReds']*df_train['meanExp']\n",
    "df_train['IATYellowCards'] = df_train['yellowCards']*df_train['meanIAT']\n",
    "df_train['ExpYellowCards'] = df_train['yellowCards']*df_train['meanExp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregation per player using 'playerShort'\n",
    "summed = df_train[['playerShort','games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']].groupby('playerShort').sum()\n",
    "meaned = df_train[['playerShort', 'skinColor','Attacking Midfielder', 'Center Back', 'Center Forward',\n",
    "       'Center Midfielder', 'Defensive Midfielder', 'Goalkeeper',\n",
    "       'Left Fullback', 'Left Midfielder', 'Left Winger', 'Right Fullback',\n",
    "       'Right Midfielder', 'Right Winger', 'England', 'France', 'Germany',\n",
    "       'Spain', 'meanIAT', 'meanExp', 'IATRedCards', 'ExpRedCards', 'IATYellowRedCards', 'ExpYellowRedCards', 'IATYellowCards', 'ExpYellowCards', 'weight', 'height' ]].groupby('playerShort').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_agg = pd.concat([summed, meaned], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_agg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for displaying feature importance\n",
    "def feature_importance(X, features, n_most_important_features=5):\n",
    "    \"\"\"\n",
    "    Prints the n most important features of the classifier in order\n",
    "    Format: (feature_name, importance)\n",
    "    @param X: The data matrix\n",
    "    @param features: clf.feature_importances_\n",
    "    @param n_most_important_features\n",
    "    \"\"\"\n",
    "    feature_importance = {}\n",
    "    for x,y in zip(X.columns,features):\n",
    "        feature_importance[x]=y\n",
    "    import operator\n",
    "    sorted_feature_importance = sorted(feature_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    for i in range(min(len(features),n_most_important_features)):\n",
    "        print (sorted_feature_importance[i])\n",
    "    \n",
    "    return sorted_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for plotting learning curves\n",
    "def plot_learning_curve(title, estimator, X, y, cv=30):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "        \n",
    "    estimator: clf\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=-1)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Baseline model, only based on cards\n",
    "\n",
    "We start off with a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_features='log2', max_depth=8, min_samples_leaf=2)\n",
    "Y = np.asarray(df_agg['skinColor'], dtype='str')\n",
    "X = df_agg[['yellowCards','yellowReds', 'redCards']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns  # Features in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Cross validation scores:')\n",
    "print(cross_val_score(clf, X, Y, cv=20, scoring='accuracy'))\n",
    "clf.fit(X,Y)\n",
    "print('Features ranked by importance:')\n",
    "feature_importance(X, clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Learning Curves (RandomForestClassifier) - Model 1\"\n",
    "\n",
    "plot_learning_curve(title, clf, X, Y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Model with all raw data that came out of preprocessing (no combined features)\n",
    "\n",
    "We decide to keep meanIAT and meanExp. As we have aggregated by player, they represent the average amount of bias the players receive over all the referees they meet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_features='log2', max_depth=8, min_samples_leaf=2)\n",
    "Y = np.asarray(df_agg['skinColor'], dtype='str')\n",
    "X = df_agg.drop(['skinColor','IATRedCards','ExpRedCards', 'IATYellowRedCards', 'ExpYellowRedCards','IATYellowCards', 'ExpYellowCards' ], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns  # Features in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Cross validation scores:')\n",
    "print(cross_val_score(clf, X, Y, cv=20, scoring='accuracy'))\n",
    "clf.fit(X,Y)\n",
    "print('Features ranked by importance:')\n",
    "dropping_order = feature_importance(X, clf.feature_importances_, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Learning Curves (RandomForestClassifier) - Model 2\"\n",
    "\n",
    "plot_learning_curve(title, clf, X, Y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Model with added features that multiply the meanIAT and the meanExp with the red/yellow/yellowred cards values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_features='log2', max_depth=8, min_samples_leaf=2)\n",
    "Y = np.asarray(df_agg['skinColor'], dtype='str')\n",
    "X = df_agg.drop(['skinColor'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns  # Features in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('Cross validation scores:')\n",
    "print(cross_val_score(clf, X, Y, cv=20, scoring='accuracy'))\n",
    "clf.fit(X,Y)\n",
    "print('Features ranked by importance:')\n",
    "feature_importance(X, clf.feature_importances_, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Learning Curves (RandomForestClassifier) - Model 3\"\n",
    "\n",
    "plot_learning_curve(title, clf, X, Y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Conclusions:\n",
    "\n",
    "By simply taking the cards as input data, we are able to make a pretty good prediction as to the skin color of the player (Model 1). This could lead to the conclusion that racial bias is a significant factor. The learning graph shows that the variance in model performance is very large for different folds in the cross-validation.\n",
    "\n",
    "By including data on the the racial biases of the referees (depending on the country they come from), we are able to improve the model by about 2 percentage points. But more importantly the model exhibits a significantly lower variance. This seems reasonable because these values show the way referees give out the cards differently to people of different color.\n",
    "\n",
    "Concerning the feature importance we see that in Model 1 that yellow cards have the greatest importance. Since giving a red card is a very strong decision it will be influenced less by racial bias. Giving a yellow card is a softer decision, thus leaving room for objectivity and influence. We can observe this by inspecting the feature importance. In Model 1 the most important feature is by far the amount of YellowCards received. In Model 2 the bias scores are the biggest influencers.\n",
    "\n",
    "This leads us to build the 3th model using a combination of the number of cards received and the bias scores. Our assumption is confirmed because the combination of bias scores with received yellow card are observed as important features. Although Model 3 does not score much higher, this leads to the conclusion that the extra features do not offer more added value than the average of meanIAT or meanExp over all refrees for a given player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering while dropping features one by one\n",
    "\n",
    "We take the dropping order based on the reversed feature importance result given by Model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_list = [x[0] for x in dropping_order]\n",
    "drop_list = drop_list[::-1]\n",
    "drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def silhouette_score_drop(df, c_drop, n_clust):\n",
    "    while len(c_drop) > 0:\n",
    "        kmeans = KMeans(n_clusters=n_clust, init='k-means++').fit(df[c_drop])\n",
    "        print(\"- Mixed percentage: \", abs(df['skinColor'] - kmeans.labels_).sum()/df.shape[0])\n",
    "        print(\"- Silhouette score: \", silhouette_score(df[c_drop], kmeans.labels_, metric='euclidean'))\n",
    "        print(\"- Confusion matrix: \\n\", confusion_matrix(df['skinColor'], kmeans.labels_))\n",
    "        print(\"------------------------------------------------\")\n",
    "        c_drop = c_drop[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silhouette_score_drop(df_agg, drop_list, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silhouette_score_drop(df_agg, drop_list, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above iterative dropping of features gives us a clustering in 2 groups with a silhouette score 0.9 where playes with dark or light skin are mixed. Being a player from France has a bigger influence on the clustering than the bias scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, init='k-means++').fit(df_agg[['France','meanIAT','meanExp']])\n",
    "print(\"- Mixed percentage: \", abs(df_agg['skinColor'] - kmeans.labels_).sum()/df_agg.shape[0])\n",
    "print(\"- Silhouette score: \", silhouette_score(df_agg[['France','meanIAT','meanExp']], kmeans.labels_, metric='euclidean'))\n",
    "print(\"- Confusion matrix: \\n\", confusion_matrix(df_agg['skinColor'], kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we can find a perfect silhouette score based on the team of theplayer where the players with different skin color are very mixed. But this feature is not a feature of the referee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, init='k-means++').fit(df_agg[['France','Germany','England','Spain']])\n",
    "print(\"- Mixed percentage: \", abs(df_agg['skinColor'] - kmeans.labels_).sum()/df_agg.shape[0])\n",
    "print(\"- Silhouette score: \", silhouette_score(df_agg[['France','Germany','England','Spain']], kmeans.labels_, metric='euclidean'))\n",
    "print(\"- Confusion matrix: \\n\", confusion_matrix(df_agg['skinColor'], kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
